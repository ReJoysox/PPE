import streamlit as st
from ultralytics import YOLO
from PIL import Image
import cv2
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="SafeGuard PRO", layout="centered")
st.title("üõ°Ô∏è SafeGuard –ò–ò")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    try:
        model = YOLO('best.onnx', task='detect')
        return model
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ best.onnx: {e}")
        return None

model = load_model()

if model:
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    st.sidebar.write("### –ö–ª–∞—Å—Å—ã –≤ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏:")
    st.sidebar.write(list(model.names.values()))
    
    conf_val = st.sidebar.slider("–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", 0.1, 1.0, 0.4)

    def process_frame(img):
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PIL -> OpenCV (BGR)
        img_cv = cv2.cvtColor(np.array(img), list(cv2.COLOR_RGB2BGR if len(np.array(img).shape)==3 else cv2.COLOR_GRAY2BGR))
        
        # –ó–∞–ø—É—Å–∫ –ò–ò
        results = model.predict(img, conf=conf_val)
        boxes = results[0].boxes
        
        found_person = False
        found_protection = False

        if len(boxes) == 0:
            return cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)

        # 1. –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤—Å—é –∑–∞—â–∏—Ç—É –∏ —Ä–∏—Å—É–µ–º –µ—ë
        for box in boxes:
            cls_id = int(box.cls[0])
            label = model.names[cls_id].lower()
            xyxy = box.xyxy[0].tolist()
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ù–ï —á–µ–ª–æ–≤–µ–∫, —Ä–∏—Å—É–µ–º –∑–µ–ª–µ–Ω—É—é —Ä–∞–º–∫—É –∑–∞—â–∏—Ç—ã
            if 'person' not in label and 'human' not in label:
                found_protection = True
                cv2.rectangle(img_cv, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0, 255, 0), 3)
                cv2.putText(img_cv, label.upper(), (int(xyxy[0]), int(xyxy[1]-10)), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            else:
                found_person = True
                # –†–∏—Å—É–µ–º —Ç–æ–Ω–∫—É—é —Ä–∞–º–∫—É –≤–æ–∫—Ä—É–≥ —á–µ–ª–æ–≤–µ–∫–∞
                cv2.rectangle(img_cv, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (255, 255, 255), 1)

        # 2. –õ–æ–≥–∏–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —á–µ–ª–æ–≤–µ–∫–∞, –Ω–æ –Ω–µ –Ω–∞—à–ª–∏ –∑–∞—â–∏—Ç—É –≤ –∫–∞–¥—Ä–µ
        if found_person and not found_protection:
            for box in boxes:
                label = model.names[int(box.cls[0])].lower()
                if 'person' in label or 'human' in label:
                    xyxy = box.xyxy[0].tolist()
                    # –ü–∏—à–µ–º –ö–†–ê–°–ù–´–ú –Ω–∞–¥ –≥–æ–ª–æ–≤–æ–π
                    cv2.putText(img_cv, "!!! NO PROTECTION !!!", (int(xyxy[0]), int(xyxy[1]-15)), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)
                    # –í—ã–¥–µ–ª—è–µ–º –≥–æ–ª–æ–≤—É –∫—Ä–∞—Å–Ω—ã–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–º
                    head_h = int((xyxy[3] - xyxy[1]) * 0.25)
                    cv2.rectangle(img_cv, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[1] + head_h)), (0, 0, 255), 2)

        return cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)

    # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–∫–ª–∞–¥–æ–∫
    t1, t2 = st.tabs(["üé• –ö–∞–º–µ—Ä–∞", "üìÅ –ó–∞–≥—Ä—É–∑–∫–∞"])

    with t1:
        cam_img = st.camera_input("–°–¥–µ–ª–∞–π—Ç–µ —Å–Ω–∏–º–æ–∫")
        if cam_img:
            res = process_frame(Image.open(cam_img))
            st.image(res, width=500)

    with t2:
        up_img = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ", type=['jpg', 'png', 'jpeg'])
        if up_img:
            res = process_frame(Image.open(up_img))
            st.image(res, width=500)
else:
    st.error("–§–∞–π–ª best.onnx –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏!")
