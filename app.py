import streamlit as st
from ultralytics import YOLO
import cv2
import numpy as np
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import av

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="SafeGuard LITE", layout="centered")
st.title("üõ°Ô∏è SafeGuard –ò–ò: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LIVE")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    return YOLO('best.onnx', task='detect')

model = load_model()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏")
camera_option = st.sidebar.radio("–ö–∞–º–µ—Ä–∞:", ("–°–µ–ª—Ñ–∏", "–û—Å–Ω–æ–≤–Ω–∞—è"))
facing_mode = "user" if camera_option == "–°–µ–ª—Ñ–∏" else "environment"
conf_val = st.sidebar.slider("–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", 0.1, 1.0, 0.5)

# --- –£–õ–¨–¢–†–ê-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.frame_count = 0
        self.last_results = None

    def recv(self, frame):
        self.frame_count += 1
        img = frame.to_ndarray(format="bgr24")

        # –•–ê–ö: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π 5-–π –∫–∞–¥—Ä, —á—Ç–æ–±—ã —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–≤–∏—Å–∞–ª
        if self.frame_count % 5 == 0:
            # imgsz=160 ‚Äî —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è –ò–ò (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ)
            results = model.predict(img, conf=conf_val, imgsz=160, verbose=False)
            self.last_results = results[0].boxes
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ—à–ª–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ‚Äî —Ä–∏—Å—É–µ–º –∏—Ö
        if self.last_results is not None:
            people = []
            protection = []

            for box in self.last_results:
                c = box.xyxy[0].tolist()
                label = model.names[int(box.cls[0])].lower()
                if 'person' in label or 'human' in label:
                    people.append(c)
                else:
                    protection.append(c)
                    cv2.rectangle(img, (int(c[0]), int(c[1])), (int(c[2]), int(c[3])), (0, 255, 0), 2)

            for p in people:
                px1, py1, px2, py2 = p
                is_safe = any(not (r[2] < px1 or r[0] > px2 or r[3] < py1 or r[1] > py2) for r in protection)
                color = (0, 255, 0) if is_safe else (0, 0, 255)
                cv2.rectangle(img, (int(px1), int(py1)), (int(px2), int(py2)), color, 1)
                if not is_safe:
                    cv2.putText(img, "NO PPE", (int(px1), int(py1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–æ–≤ Google
RTC_CONFIG = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})

webrtc_streamer(
    key="mobile-fast",
    video_processor_factory=VideoProcessor,
    rtc_configuration=RTC_CONFIG,
    media_stream_constraints={
        "video": {
            "facingMode": facing_mode,
            "width": {"max": 480}, # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞
            "frameRate": {"max": 20}
        },
        "audio": False,
    },
    async_processing=True, # –ù–µ –∂–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ò–ò –¥–ª—è –ø–æ–∫–∞–∑–∞ –≤–∏–¥–µ–æ
)

st.warning("‚ö†Ô∏è –ï—Å–ª–∏ –≤–∏–¥–µ–æ –∑–∞–≤–∏—Å–ª–æ ‚Äî –Ω–∞–∂–º–∏—Ç–µ STOP –∏ —Å–Ω–æ–≤–∞ START. –°–µ—Ä–≤–µ—Ä—É –Ω—É–∂–Ω–æ –≤—Ä–µ–º—è '–ø—Ä–æ–≥—Ä–µ—Ç—å—Å—è'.")
